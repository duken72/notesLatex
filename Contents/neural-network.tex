% !TeX spellcheck = en_US
\chapter{Neural Network}
Deep neural network takes care of the complex feature engineering process (\charef{cha:feature-engineering}). \Eg, in classical computer vision, for people detection, the following process is applied:
\begin{enumerate}
	\item Input: Image
	\item Low-level feature extraction: \ac{HOG}
	\item Mid-level features: \ac{DPM}
	\item Classifier: \ac{SVM}
	\item Output: final label
\end{enumerate}
Using deep neural network, the process is truncated to simply:
\begin{enumerate}
	\item Input: Image
	\item Network training (end-to-end)
	\item Output: final label
\end{enumerate}
Different tasks require special expertise to design feature extraction, \eg, designing a program playing gammon would need someone knowing the rules, tips and tricks. On the other hands, we can assured that the human-proposed features are sufficient and helpful. Deep neural network alleviate the human-effort of hand-designing feature extraction process. In addition, it also learns prioritize important features for specific task.

\section{General}
\hlb{Forward pass:}
\begin{align}
	\textbf{y}^{(0)} &= \textbf{x}\\
	\textbf{z}^{(k)} &= \textbf{W}^{(k)}\textbf{y}^{(k-1)}, \qquad k = 1, \dots, l\\
	\textbf{y}^{(k)} &= g_k(\textbf{z}^{(k)})\\
	\textbf{y} &= \textbf{y}^{(l)}\\
	E &= L(\textbf{t}, \textbf{y}) + \lambda \Omega(\textbf{W})
\end{align}
\hlb{Backward pass:}
\begin{align}
	h \leftarrow &\frac{\partial E}{\partial \textbf{y}} = \frac{\partial}{\partial \textbf{y}} L(\textbf{t, y}) + \lambda \frac{\partial}{\partial \textbf{y}} \Omega\\
	\text{for } k =l \leftarrow &1:\\
	h \leftarrow &\frac{\partial E}{\partial \textbf{z}^{(k)}} = h \odot g(\textbf{y}^{(k)})\\
	&\frac{\partial E}{\partial \textbf{w}^{(k)}} = h \textbf{y}^{(k-1)T} + \lambda \frac{\partial \Omega}{\partial \textbf{w}^{(k)}}\\
	h \leftarrow &\frac{\partial E}{\partial \textbf{y}^{(k-1)}} = \textbf{W}^{(k)T} h
\end{align}

\section{Gradient Descent}
\note Just use ADAM??

\subsection{Vanilla Gradient Descent}
\begin{equation}
	\theta_{t+1} = \theta_t - \eta\nabla_\theta f(\theta_t)
\end{equation}
Check derivative:
\begin{equation}
	f'(x) \approx \frac{f(x+\varepsilon) - f(x-\varepsilon)}{2\varepsilon} \;\;\;\;\; \text{(numerical gradient)}
\end{equation}

\subsection{Momentum}
\begin{itemize}
	\item Init: $v_{dW_0} = 0, v_{db_0} = 0$
	\item Calculate $dW, db$
	\item Update $W, b$
	\begin{equation}
		\Rightarrow \begin{cases}
			v_{dW} &= \beta v_{dW} + (1-\beta)dW\\
			v_{db} &= \beta v_{db} + (1-\beta)db
		\end{cases}
		\Rightarrow
		\begin{cases}
			W &= W - \alpha v_{dW}\\
			b &= b - \alpha v_{db}
		\end{cases}
	\end{equation}
	The above formulas are to calculate the moving average of $v_{dW}$ and $v_{db}$.
	\item Tips: Choose \hlre{\beta_1=0.9}, implying taking average of the last 10 steps.
	\item Reference source: \href{https://youtu.be/k8fTYJPd3_I}{DeepLearning.AI}.
\end{itemize}

\subsection{Nesterov Accelerated Gradient}
\ac{nag}:
\begin{equation}
	v_t = \gamma v_{t-1} + \eta \nabla_tJ\left(\theta - \gamma v_{t-1}\right)
\end{equation}

\subsection{RMSprop}
\ac{rmsprop}:
\begin{itemize}
	\item Init $s_{dW_0} = 0, s_{db_0}=0$
	\item Calculate $dW, db$
	\item Update $W, b$
	\begin{equation}
		\begin{cases}
			s_{dW} &= \beta s_{dW} + (1-\beta)dW^2 \\
			s_{db} &= \beta s_{db} + (1-\beta)db^2
		\end{cases}
		\Rightarrow
		\begin{cases}
			W &= W - \alpha \frac{dW}{\sqrt{s_{dW}} + \varepsilon}\\
			b &= b - \alpha \frac{db}{\sqrt{s_{db}} + \varepsilon}
		\end{cases}
	\end{equation}
	\item Tips: choose \hlre{\beta_2 = 0.999, \;\;\varepsilon = 10^{-7}}
	\item Reference source: \href{https://youtu.be/_e-LFe_igno}{DeepLearning.AI}.
\end{itemize}

\subsection{Adam}
\ac{adam} is basically the combination of Momentum and \ac{rmsprop}.
\begin{itemize}
	\item Init $v_{dW_0}, s_{dW_0}, v_{db_0}, s_{db_0}=0$
	\item Calculate $dW, db$
	\item Update $W, b$
	\begin{equation}
		\begin{cases}
			v_{dW} &= \beta_1 v_{dW} + (1-\beta_1)dW \\
			v_{db} &= \beta_1 v_{db} + (1-\beta_1)db \\
			s_{dW} &= \beta_2 s_{dW} + (1-\beta_2)dW^2 \\
			s_{db} &= \beta_2 s_{db} + (1-\beta_2)db^2
		\end{cases}
		\Rightarrow
		\begin{cases}
			v^{cor.}_{dW} &= \frac{v_{dW}}{1 - \beta_1^t} \\
			v^{cor.}_{db} &= \frac{v_{db}}{1 - \beta_1^t} \\
			s^{cor.}_{dW} &= \frac{s_{dW}}{1 - \beta_2^t} \\
			s^{cor.}_{db} &= \frac{s_{db}}{1 - \beta_2^t}
		\end{cases}
		\Rightarrow
		\begin{cases}
			W &= W - \alpha \frac{v^{cor.}_{dW}}{\sqrt{s^{cor.}_{dW}} + \varepsilon}\\
			b &= b - \alpha \frac{v^{cor.}_{db}}{\sqrt{s^{cor.}_{db}} + \varepsilon}
		\end{cases}
	\end{equation}
	\item Tips: choose \hlre{\beta_1 = 0.9, \quad\beta_2 = 0.999, \quad\varepsilon = 10^{-7}}
	\item Reference source: \href{https://youtu.be/JXQT_vxqwIs}{DeepLearning.AI}.
\end{itemize}

\section{Normalization}
\subsection{BatchNorm}
Check: \href{https://youtu.be/DtEq44FTPM4}{Batch Normalization - EXPLAINED!}

\ac{BatchNorm} normalizes the mean and standard deviation for each individual feature channel \cite{ioffe2015batch}. Given an input batch $x$ from batch size $N$, \ac{no} channel $C$, height $H$ and width $W$:

\begin{align}
	BN(x) &= \gamma \left(\frac{x - \mu(x)}{\sigma(x)}\right) + \beta, \qquad \begin{matrix*}[l]
		\text{input}\ x \in \mathbb{R}^{N \times C \times H \times W}\\
		\text{mean and standard deviation}\ \mu(x), \sigma(x) \in \mathbb{R}^C\\
		\text{affine \ac{param} to learn}\ \gamma, \beta \in \mathbb{R}^C\\
	\end{matrix*}\\
	\mu_c(x) &= \frac{1}{NHW} \sum_{n=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{nchw}\\
	\sigma_c(x)	&= \sqrt{\frac{1}{NHW} \sum_{n=1}^{N} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_nhcw - \mu_c(x))^2 + \epsilon} \label{eq:batch-norm}
\end{align}

\hlb{Intuition:}
\begin{itemize}
	\item $\gamma$ and $\beta$ are basically the learned standard deviation and mean of the output feature map.
	\item \ac{BatchNorm} layer simply transforms the output to a different mean and standard deviation.
	\item Why \ac{BatchNorm} helps?:
	\begin{itemize}
		\item Check these videos: \href{https://youtu.be/DtEq44FTPM4}{CodeEmporium}, \href{https://youtu.be/nUUqwaxLnWs}{DeepLearningAI}
		\item Speeds up training speed: by transforming the parameter space to be more even, instead of being overwhelmed by some parts of \ac{param}. This is basically the same as using \ac{adam} instead of conventional \ac{SGD}
		\item Allows suboptimal starts: same reason as above.
		\item Regularizes the model (slightly)
	\end{itemize}
\end{itemize}

\subsection{IN}
\ac{IN} \cite{ulyanov2016texture} show significant improvement over \ac{BatchNorm}, especially for the style transfer problem in \ac{CV}:
\begin{itemize}
	\item \ac{BatchNorm} computes mean and deviation for each channel: $\mu(x), \sigma(x) \in \mathbb{R}^C$ (\eqref{eq:batch-norm})
	\item \ac{IN} computes mean and standard deviation independently for each channel and each sample: $\mu(x), \sigma(x) \in \mathbb{R}^{N \times C}$
	\begin{align}
		IN(x) &= \gamma \left(\frac{x - \mu(x)}{\sigma(x)}\right) + \beta\\
		\mu_nc(x) &= \frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{nchw}\\
		\sigma_nc(x) &= \sqrt{\frac{1}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_nhcw - \mu_nc(x))^2 + \epsilon}
	\end{align}
	\item At test time, \ac{IN} layers are applied unchanged, while \ac{BatchNorm} layers usually replace mini-batch statistics with population statistics.
	\item \textit{Conditional instance normalization} (CIN) is an extension of \ac{IN} \cite{dumoulin2016learned}
	\begin{equation}
		CIN(x; s) = \gamma^s \left(\frac{x - \mu(x)}{\sigma(x)}\right) + \beta^s
	\end{equation}
	\item It's interesting that the affine parameters $\gamma, \beta$ can completely change the style of the output image.
\end{itemize}
\subsection{AdaIN}
\ac{AdaIN} simply aligns the mean and variance of content input $x$ to match those of style input $y$. Here, the affine parameters are no longer learnable, but adaptively computed from
the style input \cite{huang2017arbitrary}
\begin{equation}
	AdaIN(x,y) = \sigma(y) \left(\frac{x - \mu(x)}{\sigma(x)}\right) + \mu(y)
\end{equation}
\note Using concatenation alone would not disentangle the content and style information for later learning step.

\section{Convolutional Operator}
\subsection{Convolution}
\cite{lecun1998gradient}

\subsection{Transposed Convolution}
This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation as it does not compute a true inverse of convolution). For more information, see the visualizations here \cite{dumoulin2016guide} and the Deconvolutional Networks paper \cite{zeiler2010deconvolutional}.

\section{Tips and Tricks}
\begin{itemize}
	\item \href{https://github.com/chiphuyen/machine-learning-systems-design}{Machine Learning Systems Design}
	\item Shuffling
	\item Data Augmentation: reshape, rescale, crops, zooming, change color (color \ac{PCA})
	\item Normalizing the inputs\\
	Convergence is the fastest if
	\begin{itemize}
		\item The mean of each input variable $=0$
		\item Scale $\Rightarrow$ same covariance
	\end{itemize}
	Mean cancellation $\Rightarrow$ \ac{KL} expansion $\Rightarrow$ covariance equalization (if possible)
	\item Leaky \ac{relu} is better a bit than \ac{relu}, ELU
	\item Weights initialization: Xavier-Glorot:
	\[ W \sim U\left(0, \sqrt{\frac{6}{n_{in} + n_{out}}}\right) \]
	\item Batch Norm(alization): Normalize after each layer\\
	$\Rightarrow$ learn the moving average
	\item Drop out\\
	\note When in inferencing (after training), must multiply the activation output with the \ac{prob} that the weights are set to 0
\end{itemize}