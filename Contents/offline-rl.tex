% !TeX spellcheck = en_US
\chapter{Offline Reinforcement Learning}
One major challenge in \ac{RL} is the ability to generalize to different tasks, unlike with supervised learning. \Eg, supervised learning in \ac{CV}, a model can be trained on a large dataset (\eg, ImageNet), then retrained on a smaller dataset to adapt with a specific task. This is possible because it was proven that early layers manage to learn different feature representation What \ac{RL} differs from more conventional supervised learning are large diverse dataset, training procedure, how and when the data is collected, \etc. Offline \ac{RL} is one of proposed solution for \ac{RL} generalization problem. This chapter examines how we can formulate \ac{RL} similar to supervised learning.

\section{Formulation}
Offline \ac{RL} is also known as batch \ac{RL}, fully off-policy \ac{RL}.
\begin{figure}[hbt!]
	\centering
	\begin{subfigure}[b]{0.25\textwidth}
		\centering
		\includegraphics[width=\textwidth]{on-policy-rl.png}
		\caption{On-policy \ac{RL}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.25\textwidth}
		\centering
		\includegraphics[width=\textwidth]{off-policy-rl.png}
		\caption{Off-policy \ac{RL}}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\textwidth]{offline-rl.png}
		\caption{Offline \ac{RL}}
	\end{subfigure}
	\caption{Different settings for \ac{RL}.}
\end{figure}

Formal problem definition: Offline \ac{RL}
\begin{align}
	&\mathcal{D} = \{ (\textbf{s}_i, \textbf{a}_i, \textbf{s}'_i, \textbf{r}_i) \} &&-\text{dataset of transitions}\\
	&\textbf{s} \sim d^{\pi_\beta}(\textbf{s})\\
	&\textbf{a} \sim \pi_\beta(\textbf{a|s}) &&-\text{unknown policy}\\
	&\textbf{s}' \sim p(\textbf{s}'|\textbf{s,a})\\
	&r \leftarrow r(\textbf{s,a})\\
	&\underset{\pi}{\max} \sum_{t=0}^T \mathbb{E}_{\textbf{s}_t \sim d^{\pi_\beta}(\textbf{s}), \textbf{a} \sim \pi(\textbf{a|s})} [\gamma^t r(\textbf{s}_t, \textbf{a}_t)] &&-\text{\ac{RL} objective}
\end{align}

\begin{itemize}
	\item \ac{OPE}: given $\mathcal{D}$, estimate $\displaystyle J(\pi_\beta) = \mathbb{E}_{\pi_\beta} \left[ \sum_{t=1}^T r(\textbf{s}_t, \textbf{a}_t) \right]$
	\item Offline \ac{RL}: given $\mathcal{D}$, learn the \hlb{best possible} policy $\pi_{\theta}$, which might not be the best possible policy for the \ac{MDP}
\end{itemize}

We hope this is possible because: we expect the model can
\begin{itemize}
	\item Find the \textit{"good stuff"} in a dataset full of good and bad behaviors
	\item Generalization: good behavior in one place may suggest good behavior in another place
	\item \textit{"Stitching"}: parts of good behaviors can be recombined \cite{singh2020cog}
\end{itemize}

\section{Challenges}
\begin{itemize}
	\item Little amount of online tuning has significantly more impact than offline training \cite{kalashnikov2018qt}
	\item Distribution shift\\
	\begin{align*}
		&\theta \leftarrow \underset{\theta}{\arg\max} \mathbb{E}_{\textbf{x}\sim p(\textbf{x}), y \sim p(y|\textbf{x})} [(f_\theta(\textbf{x})-y)^2] && -\text{normal regression problem}\\
		&\mathbb{E}_{\textbf{x}'\sim p(\textbf{x})} [(f_\theta(\textbf{x}')-y)^2] &&-\text{is low in expectation}\\
		&\mathbb{E}_{\textbf{x}'\sim \bar{p}(\textbf{x})} [(f_\theta(\textbf{x}')-y)^2] &&-\text{is not, for general $\bar{p}(\textbf{x}) \neq p(\textbf{x})$}\\
		&\mathbb{E}_{\textbf{x}'\sim p(\textbf{x})} [(f_\theta(\textbf{x}')-y)^2] &&-\text{is \hlb{NOT} low if $\textbf{x}' \leftarrow \underset{\textbf{x}}{\arg\max} f_\theta(\textbf{x})$}\\
	\end{align*}
	This is exactly what happens for value-based \ac{RL} approaches, in which we decide the action based on maximizing the value function $\Rightarrow$ Over-estimation of value function \cite{kumar2019stabilizing}
	\item Counterfactual queries: must account for out-of-distribution actions, ideally in a safe way, while still making use of generalization to come up with behaviors that are better than the best thing seen in the data \cite{levine2020offline}
	\item Issues with generalization are not corrected:
	\begin{figure}[hbt!]
		\centering
		\includegraphics[width=.7\textwidth]{offline-rl-generalization-isse.png}
	\end{figure}	
\end{itemize}

\section{Classic Offline RL}
If you want to use offline \ac{RL} today, you probably would not use approaches in this section.

\subsection{Batch RL via Importance Sampling}
Batch \ac{RL} is just an old name for offline \ac{RL} (around 2000s). Using importance sampling, we can use samples from one policy to learn another one (\secref{sec:off-policy-policy-gradient})

\begin{align*}
	&\underset{\pi}{\max} \sum_{t=0}^T \mathbb{E}_{\textbf{s}_t \sim d^\pi(\textbf{s}), \textbf{a}_t \sim \pi(\textbf{a|s})} [\gamma^t r(\textbf{s}_t, \textbf{a}_t)] \qquad\qquad-\text{\ac{RL} objective}\\
	\nabla_\theta J(\theta) &= \mathbb{E}_{\tau \sim \pi_\theta(\tau)} \left[ \sum_{t=0}^T \nabla_\theta \gamma^t \log \pi(\textbf{a}_t | \textbf{s}_t) \hat{Q}(\textbf{s}_t, \textbf{a}_t) \right] \qquad\qquad-\text{policy gradient}\\
	&\approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \nabla_\theta \gamma^t \log \pi_\theta(\textbf{a}_{t,i} | \textbf{s}_{t,i}) \hat{Q}(\textbf{s}_{t,i}, \textbf{a}_{t,i}) \qquad\text{requires sampling from $\pi_\theta$}\\
	\nabla_\theta J(\theta) &\approx \frac{1}{N} \sum_{i=1}^N \underbrace{\frac{\pi_\theta(\tau_i)}{\pi_\beta(\tau_i)}}_{\text{importance weight}} \sum_{t=0}^T \nabla_\theta \gamma^t \log \pi_\theta(\textbf{a}_{t,i} | \textbf{s}_{t,i}) \hat{Q}(\textbf{s}_{t,i}, \textbf{a}_{t,i}) \qquad\text{use samples from $\pi_\beta$}\\
	\frac{\pi_\theta(\tau)}{\pi_\beta(\tau)} &= \frac{\cancel{p(\textbf{s}_1)}}{\cancel{p(\textbf{s}_1)}} \frac{\cancel{\prod_t p(\textbf{s}_{t+1} | \textbf{s}_t, \textbf{a}_t)} \pi_\theta(\textbf{a}_t | \textbf{s}_t)}{\cancel{\prod_t p(\textbf{s}_{t+1} | \textbf{s}_t, \textbf{a}_t)} \pi_\beta(\textbf{a}_t | \textbf{s}_t)} \qquad\qquad \text{exponential in $T$}\\
	\hat{Q}(\textbf{s}_{t,i}, \textbf{a}_{t,i})	&= \mathbb{E}_{\pi_\theta} \left[ \sum_{t' = t}^T \gamma^{t'-t} r_{t'} \right] \approx \sum_{t' = t}^T \gamma^{t'-t} r_{t',i} \qquad\qquad\text{single sample estimate}\\
	\Rightarrow \nabla_\theta J(\theta) &\approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \parunderbrace{\cancel{\left( \prod_{t'=0}^{t-1} \frac{\pi_\theta(\textbf{a}_{t',i} | \textbf{s}_{t',i})}{\pi_\beta(\textbf{a}_{t',i} | \textbf{s}_{t',i})} \right)}}{accounts for difference in \ac{prob} of landing in $\textbf{s}_{t,i}$, we have $\textbf{s}_t \sim d^{\pi_\beta}(\textbf{s}_t)$, but want $\textbf{s}_t \sim d^{\pi_\theta}(\textbf{s}_t)$} \nabla_\theta \gamma^t \log \pi_\theta (\textbf{a}_{t,i} | \textbf{s}_{t,i}) \parunderbrace{\left( \prod_{t'=t}^{T} \frac{\pi_\theta(\textbf{a}_{t',i} | \textbf{s}_{t',i})}{\pi_\beta(\textbf{a}_{t',i} | \textbf{s}_{t',i})} \right)}{accounts for having incorrect $\hat{Q}(\textbf{s}_{t,i}, \textbf{a}_{t,i})$} \hat{Q}(\textbf{s}_{t,i}, \textbf{a}_{t,i})
\end{align*}

\todo{Ignore for now, too many complicated equations}

\subsection{Batch RL via Linear Fitted Value Functions}
Let's go back to the model-based \ac{RL} setting with tabular linear fitted value functions. This method generally doesn't concern with distributional shift, maybe it was not such a big problem with linear models.

High level idea procedure:
\begin{enumerate}
	\item Estimate the reward
	\item Estimate the transitions
	\item Recover the value function
	\item Improve the policy
\end{enumerate}

\begin{align}
	&\Phi &&-\text{feature matrix, size} |S| \times K\\
	&\Phi \textbf{w}_r \approx r &&-\text{reward model}, \textbf{w}_r \in \mathbb{R}^{K\times 1}, r \in \mathbb{R}^{|S| \times 1}\\
	\Rightarrow & \textbf{w}_r = (\Phi^T \Phi)^{-1} \Phi^T \vec{\textbf{r}} &&-\text{least square solution (assume knowing $\vec{\textbf{r}}$)}\\
	&\Phi \textbf{P}_\Phi \approx \textbf{P}^\pi \Phi &&-\text{transition model}\\
	&\textbf{P}^\pi \in \mathbb{R}^{|S|\times|S|} &&-\text{real transition matrix (on states)}\\
	&\textbf{P}_\Phi \in \mathbb{R}^{K\times K} &&-\text{estimated feature-space transition matrix}\\
	\Rightarrow & \textbf{P}_\Phi = \left(\Phi^T \Phi\right)^{-1} \Phi^T \textbf{P}^\pi \Phi && -\text{least squares}\\
	\Rightarrow & V^\pi \approx V^\pi_\Phi = \Phi \textbf{w}_V &&-\text{value functions}
\end{align}

We can find $V^\pi$ in terms of $\textbf{P}^\pi$ and $\textbf{r}$:
\begin{align}
	V^\pi &= \textbf{r} + \gamma \textbf{P}^\pi V^\pi\\
	\Rightarrow V^\pi &= (\textbf{I} - \gamma \textbf{P}^\pi)^{-1} \textbf{r}
\end{align}
Same derivation in feature space:
\begin{align}
	\textbf{w}_V &= (\textbf{I} - \gamma \textbf{P}_\Phi)^{-1} \textbf{w}_r &&\\
	&= (\textbf{I} - \gamma (\Phi^T \Phi)^{-1} \Phi^T \textbf{P}^\pi \Phi)^{-1} (\Phi^T \Phi)^{-1} \Phi^T \vec{\textbf{r}} &&-\text{substitute in $\textbf{w}_r$ and $\textbf{P}_\Phi$}\\
	\Rightarrow \textbf{w}_V &=(\Phi^T\Phi - \gamma\Phi^T \textbf{P}^\pi \Phi)^{-1} \Phi^T \vec{\textbf{r}} &&-\text{LSTD}
\end{align}
The above equations are of \ac{LSTD}. However it requires us to know transition matrix $\textbf{P}^\pi$ and reward vector $\vec{\textbf{r}}$. To get a model-free approach, we will use samples. We replace $\textbf{P}^\pi \Phi$ with the feature matrix of the next time step $\Phi'$:
\begin{align}
	\Phi'_i = \phi(\textbf{s}'_i)
\end{align}

\hlb{\ac{LSTD} Algorithm:} not going to work for offline \ac{RL}, because the equation requires samples from $\pi$ for $\textbf{P}^\pi$
\begin{enumerate}
	\item \tikzmark{lstd1}$\pi'(\textbf{s}) \leftarrow \text{Greedy}(\Phi \textbf{w}_V)$
	\item \tikzmark{lstd2}estimate $V^{\pi'}$
	\begin{tikzpicture}[overlay,remember picture]
		\draw[very thick, -latex]
		([xshift=-7mm,yshift=1mm]pic cs:lstd2) --++ (-.5,0) |-
		([xshift=-7mm,yshift=1mm]pic cs:lstd1);
	\end{tikzpicture}
\end{enumerate}

\hlb{\ac{LSPI} Algorithm:} \ac{LSTD}Q - \ac{LSTD} for Q-functions.
\begin{enumerate}
	\item \tikzmark{lspi1}compute $\textbf{w}_Q$ for $\pi_k$
	\item $\pi_{k+1}(\textbf{s}) = \underset{\textbf{a}}{\arg\max} \phi(\textbf{s,a}) \textbf{w}_Q$
	\item \tikzmark{lspi3}set $\Phi_i' = \phi(\textbf{s}_i', \pi_{k+1}(\textbf{s}_i'))$
	\begin{tikzpicture}[overlay,remember picture]
		\draw[very thick, -latex]
		([xshift=-7mm,yshift=1mm]pic cs:lspi3) --++ (-.5,0) |-
		([xshift=-7mm,yshift=1mm]pic cs:lspi1);
	\end{tikzpicture}
\end{enumerate}
Now $\Phi \in \mathbb{R}^{|S||A| \times K'}$, $\textbf{w}_Q = (\Phi^T \Phi - \gamma \Phi^T \Phi')^{-1} \Phi^T \vec{\textbf{r}}$, $\vec{\textbf{r}}_i =r(\textbf{s}_i, \textbf{a}_i)$ \cite{lagoudakis2003least}

\section{Conservative Q-Learning}
\ac{CQL}

\section{Model-Based Offline RL}
\ac{MOPO}

\section{Summary}
\section{Applications}
\section{Open Questions}

\section{References}
Readings for importance sampling
\begin{itemize}
	\item Classic work on importance sampled policy gradients and return estimation:
	\begin{itemize}
		\item \citeaustitle{precup2000eligibility}
		\item \citeaustitle{peshkin2002learning}
	\end{itemize}
	\item Doubly robust estimators and other improved importance-sampling estimators:
	\begin{itemize}
		\item \citeaustitle{jiang2016doubly}
		\item \citeaustitle{thomas2016data}
	\end{itemize}	
	\item Analysis and theory:
	\begin{itemize}
		\item \citeaustitle{thomas2015high}
	\end{itemize}
	\item Marginalized importance sampling:
	\begin{itemize}
		\item \citeaustitle{hallak2017consistent}
		\item \citeaustitle{liu2019off}
	\end{itemize}
\end{itemize}

Modern offline \ac{RL}:
\begin{itemize}
	\item \citeaustitle{levine2020offline}
\end{itemize}