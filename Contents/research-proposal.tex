% !TeX spellcheck = en_US
\chapter{Research Proposal}

\section{Introduction and Background of Interest}

\todo{ignore for now}

Robots are complex machines designed to support our lives.

Since mid 19th century, its emergence has received research attention in both the academic and industry. The first major application was in the automotive industry. Its presence is entering our lives in different fields is spreading even more and more.

Its appearance has improved 

\section{Literature Review}

To my best current knowledge, robot arms have made significant improvement, but are still far from delivering general-purpose tasks. Initially, robot arms are programmed explicitly and only capable of working in the known and constrained environment of factories. Progress in different fields of technology has provided the robots more inputs (\eg, vision, audio, haptic) to operate in dynamics and unstructured environment. State-of-the-art robot arm models can deliver complex human tasks, \eg, cooking \cite{moley}, making coffee \cite{coffeemaster}, and cleaning around the house \cite{bothandy}. However, the behaviors of these models are still awkwardly disruptive and far from the mastery level of human hand's dexterity. In addition, instead of having the adaptability for a wide range of working conditions, most models still operate in designed environments with known tools and configurations.

\subsection{Robot Grasping}

Grasping is one of the fundamental and unavoidable problems for robot arms, whether for the logistics or service robots. Most successful approaches are empirical, using deep learning on large dataset to find the best grasping representations. Majority of models are supervised training using single-modal input, \ie RGB images. A few has considered multi-modal input: RGB with depth images, tactile input. \cite{caldera2018review}

\todo{previous grasping approach that use tactile input?}

\cite{calandra2017feeling} 

\subsection{Robot Learning}
Works on reaching target, handling objects

the data gap between different approaches: some with million of samples (Sanctuary AI) in simulation, some with collective robots learning (google) $\rightarrow$ a data-efficient approach for grasping

efficient and smoothing object grasping and manipulation would concerns with both both visual and force sensor. Thus input from computer vision and tactile sensor have to be integrated \cite{haddadin2018tactile}

The gap between what robots can utilize and what human can utilize

faster learning from demonstration

different type of demonstration (visual, instead of guiding physically)

A meta learning approach for grasping by human demonstration. Prior works approach:
Given 3D models, using advanced physical simulation, ...
sensing for grasping
examining object shape, geometries, center of gravity, \etc

a meta learning approach may also be applicable to non-rigid, deformable object (\eg, clothes, towels)

I argue that it's irrelevant to learn and optimize where to grasp an object or the object characteristics. The optimal grasp depends on the usage of the tools, objects. Someday the robot will have more context about what all tools/objects are for, but, a few demonstration would be straight forward. Thus, I want to develop a model that able to input only a few of human demonstration and be able to learn how to grasp the object appropriately.
plan to approach this problem with meta learning. Meta learning is also helpful for transfer learning experience to other task, deal with the current lack of large available dataset for different arms and different tasks.

With the current attention on 3D CV, I believe that soon there will be a approach to extract more information regarding object geometries, materials and usage context. Thus, a multi-modal approach would be nice to pave the way later.

a mapping from human arm behavior to robot arm behavior. helpful for different tasks, either grasping, object manipulation, pick and place

\section{Approaches and Choice of Methods}
\subsection{Key Points for Improvement}
The progress I wish to work towards concerns robotic collective learning for collaborative tasks. For example, a system of multiple robot arms playing 2v2 table tennis, packaging an item or cooking. Building up towards the mentioned progress, the improvement points I want to make are as follows:

\begin{itemize}
	\item Extend the usage of tactile sensors to learning object grasping and manipulation. Tactile sensor has mostly been used for the development of safety robot / soft robotics \cite{haddadin2018tactile}. Previous works using tactile sensor on robotic grasping focus on \todo{sth, read UC Bekerly's paper}
	\item Robotic grasping mostly with rigid object.
	\item Meta \ac{RL} / generalization problem of \ac{RL}
	\item Data efficiency problem
\end{itemize}

[\textbf{FORMAT:} Prior works suffer from, limited, make assumptions on, I want to extend \dots]

\subsection{Approaches and Research Contributions}
Directions / smaller problems I want to pursue:

\begin{itemize}
	\item A data collection pipeline for robot arms. Preferably a mapping to imitate human’s movements, which, at the same time, collecting visual and haptic information.
	\item Integrate tactile information to robotic object grasping. Combining with visual data, process the haptic data to extract other possible object's characteristics, \eg, weight, material, deformability.
	This will then possibly extend to some learning on object manipulation.
	\item An approach for faster, purposeful exploration and generalization: not necessarily exploration from scratch, but based on some prior expert inputs.
	\item 
\end{itemize}
Integration with generative models. Could generative models be incorporated effectively into RL algorithms? They can perhaps generate expert-like samples for imitation learning. Should and how can we evaluate the importance of different samples?

sth can be broken down into different approaches, sub-problem, aspects, \dots

I intend to clarify these questions, extend these directions, improve \dots

simplified models for rigid, liquid, deformable objects

Prior work mostly concern with rigid objects.
tactile inputs would bring adaptability, optimal grip force, and perhaps more underlying information regarding material, \etc. for non-rigid, soft, deformable object. dynamics load

in addition, it would enable more complex object manipulation tasks that computer vision alone would be insufficient

\begin{itemize}
	\item A approach to map human behaviors to single-arm manipulator. Use for visual and tactile data collection. Possibly generalized for different arm manipulator, gripper, sensor input, \etc. As demonstration for other approach, imitation learning, object grasping, reaching, object manipulation, \etc
	\item Robotic grasping as few-shot meta-\ac{RL}
\end{itemize}

I also curious with the usage of some deep learning techniques: generative model, score matching, \etc.

\section{Proposed Research Plan}

\todo{}

\todo{How the IAS lab is related to this research plan?}
\begin{itemize}
	\item There is alignment in our directions
	\item t thay vi tri m tuyen phu hop vs t => nen t muon join
	\item It’s amazing that there is a great diversity at IAS, not just in terms of backgrounds, but also academic focuses. Each person works on different topics, MCTS, robot dynamics, imitation learning, etc. Regardless, different works support others and the whole team is pushing the frontier of robotics.
\end{itemize}

\todo{Give compliments to their works!}
\begin{itemize}
	\item mention more explicitly regarding the topics, what research they have done
	\item I read your papers about … let alone the works on …
\end{itemize}
