% !TeX spellcheck = en_US
\chapter{Research Proposal}

\section{Introduction and Background of Interest}

\todo{ignore for now}

Robots are complex machines designed to support our lives.

Since mid 19th century, its emergence has received research attention in both the academic and industry. The first major application was in the automotive industry. Its presence is entering our lives in different fields is spreading even more and more.

Its appearance has improved 

\section{Literature Review}

To my best current knowledge, robot arms have made significant improvement, but are still far from delivering general-purpose tasks. Initially, robot arms are programmed explicitly and only capable of working in the known and constrained environment of factories. Progress in different fields of technology has provided the robots more inputs (\eg, vision, audio, haptic) to operate in dynamics and unstructured environment. State-of-the-art robot manipulators can deliver complex human tasks, \eg, cooking \cite{moley}, making coffee \cite{coffeemaster}, and cleaning around the house \cite{bothandy}. However, the behaviors of these models are still awkwardly disruptive and far from the mastery level of human hand's dexterity. In addition, instead of having the adaptability for a wide range of working conditions, most models still operate in designed environments with known tools and configurations.

\subsection{Robot Grasping}

Grasping is one of the fundamental and unavoidable problems for robot arms, whether for the logistics or service robots. Most successful approaches are empirical, using deep learning on large dataset to find the best grasping representations. Majority of models are supervised training using single-modal input, \ie RGB images. A few has considered multi-modal input: RGB with depth images, tactile input. \cite{caldera2018review, li2019review}

\todo{previous grasping approach that use tactile input?}
\begin{itemize}
	\item Single-modal: grasp stability, adaptability to object's weight \cite{bekiroglu2011assessing, li2014learning}
	\item Multi-modal: \ac{prob} of successful grasp \cite{calandra2017feeling}
\end{itemize}

\cite{calandra2017feeling} use multi-modal data, but the tactile input is simply image-like and the task is to predict prob of successful grasp, nothing related to force, torque

Robotic grasping mostly with rigid object.

\subsection{Robot Learning}
Works on reaching target, handling objects

the data gap between different approaches: some with million of samples (Sanctuary AI) in simulation, some with collective robots learning (google) $\rightarrow$ a data-efficient approach for grasping

efficient and smoothing object grasping and manipulation would concerns with both both visual and force sensor. Thus input from computer vision and tactile sensor have to be integrated \cite{haddadin2018tactile}

The gap between what robots can utilize and what human can utilize

faster learning from demonstration

different type of demonstration (visual, instead of guiding physically)

A meta learning approach for grasping by human demonstration. Prior works approach:
Given 3D models, using advanced physical simulation, ...
sensing for grasping
examining object shape, geometries, center of gravity, \etc

a meta learning approach may also be applicable to non-rigid, deformable object (\eg, clothes, towels)

I argue that it's irrelevant to learn and optimize where to grasp an object or the object characteristics. The optimal grasp depends on the usage of the tools, objects. Someday the robot will have more context about what all tools/objects are for, but, a few demonstration would be straight forward. Thus, I want to develop a model that able to input only a few of human demonstration and be able to learn how to grasp the object appropriately.
plan to approach this problem with meta learning. Meta learning is also helpful for transfer learning experience to other task, deal with the current lack of large available dataset for different arms and different tasks.

With the current attention on 3D CV, I believe that soon there will be a approach to extract more information regarding object geometries, materials and usage context. Thus, a multi-modal approach would be nice to pave the way later.

\section{Approaches and Choice of Methods}
\subsection{Key Points for Improvement}
[\textbf{FORMAT:} Prior works suffer from, limited, make assumptions on \dots]

Prior works for robotic manipulators were limited on their utilization of tactile sensor. Complex manipulation tasks, \ie, using chopsticks, ironing, require some level of force and pressure feedback. Thus, these sensors can potentially leverage the manipulators to the mastery level as of human's arms. However, tactile sensors are currently been used mostly for the development of soft robotics \cite{haddadin2018tactile}. For grasping task, despite certain successes from using visual and tactile input separately, there hasn't been a multi-modal approach capable of combining the strength of both side, \ie, accurate localization, adaptability to object weight and deformability. A model, which has knowledge from both visual and tactile sources, promises to deliver complex manipulation tasks with diverse objects.

On the other hand, generalization is still a major challenge for \ac{RL}. 

Based on previous progresses on teleoperation \cite{handa2020dexpilot}, visual imitation learning \cite{finn2017one, sharma2019third}, visual imitation learning and meta-\ac{RL} can be extended further to learn more generalized and complex tasks. \todo{say something about previous work limitations}. In addition, incorporating tactile input is of course an obvious direction for improvement.

Prior \ac{RL} works on robotic grasping are value-based approaches, policy gradient, and make certain assumption about object's characteristics. \cite{li2019review}

\subsection{Approaches and Research Contributions}
[\textbf{FORMAT:} I want to extend, clarify these questions, extend these directions, improve \dots]

The progress I wish to work towards concerns robotic collective learning for collaborative tasks. For example, a system of multiple robot arms playing 2v2 table tennis, packaging an item or cooking. Building up towards the mentioned progress, the improvement points I want to make are as follows:

\begin{itemize}
	\item More diverse object grasping and manipulation
	\item The use of tactile input in multi-modal deep learning.
	\item Transfer learning for \ac{RL}
	\item Data efficiency for \ac{RL} with meta-\ac{RL}
	\item Few shot learning with meta RL
	\item Fast RL
	\item Visual imitation learning: learning by visual demonstration \cite{handa2020dexpilot}
	\item Exploration from demonstration
\end{itemize}

Directions / smaller problems I want to pursue:

\begin{itemize}
	\item A data collection pipeline for robot manipulators: preferably as a mapping from human’s arm movements. The data contains both visual and tactile information. It would then be used for the learning of other tasks, \eg grasping, object sensing, manipulation.
	\item Integrate tactile information to robotic object grasping. Combining with visual data, process the tactile data to extract possible object's characteristics, \eg, weight, material, deformability.
	This will then possibly extend to some learning on object manipulation.
	\item An approach for faster, purposeful exploration and generalization: not necessarily exploration from scratch, but based on some prior expert inputs.
	\item A meta-\ac{RL} approach for robotic grasping with few-shot learning. A meta learning approach could be 
	\item Learning from visual demonstration \todo{}
\end{itemize}
Integration with generative models. Could generative models be incorporated effectively into RL algorithms? They can perhaps generate expert-like samples for imitation learning. Should and how can we evaluate the importance of different samples?

sth can be broken down into different approaches, sub-problem, aspects, \dots

Prior work mostly concern with rigid objects.
tactile inputs would bring adaptability, optimal grip force, and perhaps more underlying information regarding material, \etc. for non-rigid, soft, deformable object. dynamics load

in addition, it would enable more complex object manipulation tasks that computer vision alone would be insufficient

\begin{itemize}
	\item A approach to map human behaviors to single-arm manipulator. Use for visual and tactile data collection. Possibly generalized for different arm manipulator, gripper, sensor input, \etc. As demonstration for other approach, imitation learning, object grasping, reaching, object manipulation, \etc
	\item Robotic grasping as few-shot meta-\ac{RL}
\end{itemize}

I also curious with the usage of some deep learning techniques: generative model, score matching, \etc.

\section{Proposed Research Plan}

\todo{}

\todo{How the IAS lab is related to this research plan?}
\begin{itemize}
	\item There is alignment in our directions
	\item t thay vi tri m tuyen phu hop vs t => nen t muon join
	\item It’s amazing that there is a great diversity at IAS, not just in terms of backgrounds, but also academic focuses. Each person works on different topics, MCTS, robot dynamics, imitation learning, etc. Regardless, different works support others and the whole team is pushing the frontier of robotics.
\end{itemize}

\todo{Give compliments to their works!}
\begin{itemize}
	\item mention more explicitly regarding the topics, what research they have done
	\item I read your papers about … let alone the works on …
\end{itemize}
