% !TeX spellcheck = en_US
\chapter{Research Proposal}

\section{Introduction and Background of Interest}
Robots are complex machines designed to support our lives.

Since mid 19th century, its emergence has received research attention in both the academic and industry. The first major application was in the automotive industry. Its presence is entering our lives in different fields is spreading even more and more.

Its appearance has improved 

\section{Literature Review}

To my best current knowledge, robot arms have made significant improvement, but are still far from delivering general-purpose tasks. Initially, robot arms are programmed explicitly and only capable of working in the known and constrained environment of factories. Progress in different fields of technology has provided the robots more inputs (\eg, vision, audio, haptic) to operate in more dynamics and challenging environment. State-of-the-art robot arm models can deliver complex human tasks, \eg, cook \cite{moley}, make coffee \cite{coffeemaster}, and clean around the house \cite{bothandy}. However, the behaviors of these models are still awkwardly disruptive and far from the mastery level of human hand's dexterity. In addition, instead of having the adaptability for a wide range of working conditions, most models still operate in designed environments with known tools and configurations.

\subsection{Robot Grasping and Tactile Sensor}

Grasping object has always been a fundamental problem for robot arms, whether for industrial or service robots. It is a unavoidable task for pick-and-place task and object manipulation. Prior works approach the grasping problem as finding a grasping representation or grasping position. The most successful approaches are empirical, which use deep learning on large dataset to find the best grasping positions or grasping representations. 
\cite{caldera2018review}

and use RGB images. Recent works start to dive into multi-modal model, but mostly still for RGB-D images. Only a few consider tactile input and it's still  unclear if it bring significant benefit for grasping.
\cite{calandra2017feeling} 

\subsection{Robot Learning}
Works on reaching target, handling objects

the data gap between different approaches: some with million of samples (Sanctuary AI) in simulation, some with collective robots learning (google) $\rightarrow$ a data-efficient approach for grasping

efficient and smoothing object grasping and manipulation would concerns with both both visual and force sensor. Thus input from computer vision and tactile sensor have to be integrated \cite{haddadin2018tactile}

The gap between what robots can utilize and what human can utilize

faster learning from demonstration

different type of demonstration (visual, instead of guiding physically)

A meta learning approach for grasping by human demonstration. Prior works approach:
Given 3D models, using advanced physical simulation, ...
sensing for grasping
examining object shape, geometries, center of gravity, \etc

a meta learning approach may also be applicable to non-rigid, deformable object (\eg, clothes, towels)

I argue that it's irrelevant to learn and optimize where to grasp an object or the object characteristics. The optimal grasp depends on the usage of the tools, objects. Someday the robot will have more context about what all tools/objects are for, but, a few demonstration would be straight forward. Thus, I want to develop a model that able to input only a few of human demonstration and be able to learn how to grasp the object appropriately.
plan to approach this problem with meta learning. Meta learning is also helpful for transfer learning experience to other task, deal with the current lack of large available dataset for different arms and different tasks.

With the current attention on 3D CV, I believe that soon there will be a approach to extract more information regarding object geometries, materials and usage context. Thus, a multi-modal approach would be nice to pave the way later.

a mapping from human arm behavior to robot arm behavior. helpful for different tasks, either grasping, object manipulation, pick and place

\section{Approaches and Choice of Methods}
\subsection{Key Points for Improvement}
Prior works suffer from, is limited on, make assumptions on \dots
\subsection{Approaches and Research Contributions}
sth can be broken down into different approaches, sub-problem, aspects, \dots

I intend to clarify these questions, extend these directions, improve \dots

pipeline to mapping human arm behavior to robot arm behavior, movement, grasping object

simplified models for rigid, liquid, deformable objects

\section{Proposed Research Plan}
The progress I want to make in robotics, which innately goes back to my bachelor’s thesis, concerns the learning of multiple robots for collaborative tasks. For example, a system of two (mobile) robot arms playing 2v2 table tennis, or multiple robot arms carrying on packaging tasks. I wonder, whether there should be a single unified controller or multiple separated ones, how to teach them to delegate different subtasks, will the model optimize each arm with one single subtask or switch between different ones.

The above is a complex long-term challenge. To gradually formalize and break down the problem, there are still knowledge gaps for me to fill. Some specific problems I want to research are:
A pipeline for robot arms to imitate human’s movements: concerns with pose estimation, controller designs.
An approach for faster, purposeful exploration and generalization: not necessarily exploration from scratch, but based on some prior expert inputs.

Integration with generative models: In CV, these models can already output quite realistic samples. Could generative models be incorporated effectively into RL algorithms? They can perhaps generate expert-like samples for imitation learning. Should and how can we evaluate the importance of different samples?
Increase the efficiency of the robot arm’s movement, in terms of speed and accuracy: integration of time metric into the reward function, data from tactile sensors.

I read your papers about … let alone the works on …

There is alignment in our directions

There is an African quote: “If you want to go fast, go alone. If you want to go far, go together”. Robotics and ML are interdisciplinary. This makes the study of RL even more challenging, yet immensely appealing. Thus, it’s amazing that there is a great diversity at IAS, not just in terms of backgrounds, but also academic focuses. Each person works on different topics, MCTS, robot dynamics, imitation learning, etc. Regardless, different works support others and the whole team is pushing the frontier of robotics. I believe that my direction aligns with the team’s to great extent. It would mean a lot for me, to be a part of a team, to learn and cooperate with others.

give compliment to their works
t thay vi tri m tuyen phu hop vs t => nen t muon join
mention more explicitly regarding the topics, what research they have done
