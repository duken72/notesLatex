% !TeX spellcheck = en_US
\chapter{Network Structures}
This chapter presents some common network structures and their capabilities.

\section{Base Structures}
\subsection{LeNet}
LeNet5 is probably the earliest \ac{CNN} network, proposed by \citeausm{lecun1998gradient}.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{lenet.png}
	\caption{LeNet5 Architecture. \cite{lecun1998gradient}}
\end{figure}

Example coding with \texttt{pytorch} (\href{https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/}{src}):
\begin{python}
import torch.nn as nn
	
class LeNet5(nn.Module):
	def __init__(self, num_classes):
		super(ConvNeuralNet, self).__init__()
		self.conv1 = nn.Sequential(
		nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),
		nn.BatchNorm2d(6),
		nn.ReLU(),
		nn.MaxPool2d(kernel_size = 2, stride = 2))
		self.conv2 = nn.Sequential(
		nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),
		nn.BatchNorm2d(16),
		nn.ReLU(),
		nn.MaxPool2d(kernel_size = 2, stride = 2))
		self.fc1 = nn.Sequential(
		nn.Linear(400, 120),
		nn.ReLU())
		self.fc2 = nn.Sequential(
		nn.Linear(120, 84),
		nn.ReLU())
		self.fc3 = nn.Linear(84, num_classes)
	
	def forward(self, x):
		...
		return y
\end{python}

\subsection{AlexNet}
The classic \ac{CNN} architecture for image classification on the CIFAR10 dataset \cite{krizhevsky2012imagenet}.

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{alexnet.png}
	\caption{AlexNet architecture. \cite{krizhevsky2012imagenet}}
\end{figure}

Example coding with \texttt{pytorch} (\href{https://blog.paperspace.com/alexnet-pytorch/}{src}):
\begin{python}	
class AlexNet(nn.Module):
	def __init__(self, num_classes=10):
		super(AlexNet, self).__init__()
		self.layer1 = nn.Sequential(
		nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),
		nn.BatchNorm2d(96),
		nn.ReLU(),
		nn.MaxPool2d(kernel_size = 3, stride = 2))
		...
		self.fc1 = nn.Sequential(
		nn.Dropout(0.5),
		nn.Linear(4096, 4096),
		nn.ReLU())
		self.fc2= nn.Sequential(
		nn.Linear(4096, num_classes))
	
	def forward(self, x):
		...
		return y
\end{python}

\subsection{VGG Net}
The runner-up at the ILSVRC 2014 competition by \citeaus{simonyan2014very}:

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{vggnet.png}
	\caption{The VGG Net architecture with 13 \ac{CONV} layers and 3 \ac{FC} layers. \cite{simonyan2014very}}
\end{figure}

Example coding with \texttt{pytorch} (\href{https://blog.paperspace.com/vgg-from-scratch-pytorch/}{src}):
\begin{python}
	class VGG16(nn.Module):
	def __init__(self, num_classes=10):
	super(VGG16, self).__init__()
	self.layer1 = nn.Sequential(
	nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
	nn.BatchNorm2d(64),
	nn.ReLU())
	...
	self.fc1 = nn.Sequential(
	nn.Dropout(0.5),
	nn.Linear(4096, 4096),
	nn.ReLU())
	self.fc2= nn.Sequential(
	nn.Linear(4096, num_classes))
	
	def forward(self, x):
	...
	return y
\end{python}

\subsection{Residual Network}
The \ac{ResNet} by \citeausm{he2016deep} tackles the vanishing gradient problem for deep neural network with the residual block (\figref{fig:residual-modul}). It creates a highway pass for the gradient to go to the early layers.
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.5\textwidth]{residual-modul.png}
	\caption{Residual block \cite{he2016deep}.}
	\label{fig:residual-modul}
\end{figure}

Example coding with \texttt{pytorch} (\href{https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/}{src}):
\begin{python}
class ResidualBlock(nn.Module):
	def __init__(self, in_channels, out_channels,
			stride = 1, downsample = None):
		super(ResidualBlock, self).__init__()
		self.conv1 = nn.Sequential(
		nn.Conv2d(in_channels, out_channels, kernel_size = 3,
		stride = stride, padding = 1),
		nn.BatchNorm2d(out_channels),
		nn.ReLU())
		self.conv2 = nn.Sequential(
		nn.Conv2d(out_channels, out_channels, kernel_size = 3,
		stride = 1, padding = 1),
		nn.BatchNorm2d(out_channels))
		self.downsample = downsample
		self.relu = nn.ReLU()
		self.out_channels = out_channels
	
	def forward(self, x):
		residual = x
		out = self.conv1(x)
		out = self.conv2(out)
		if self.downsample:
			residual = self.downsample(x)
		out += residual
		out = self.relu(out)
		return out
\end{python}

\begin{python}
class ResNet(nn.Module):
	def __init__(self, block, layers, num_classes = 10):
		super(ResNet, self).__init__()
		self.inplanes = 64
		self.conv1 = nn.Sequential(
		nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),
		nn.BatchNorm2d(64),
		nn.ReLU())
		self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
		self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)
		self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)
		self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)
		self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)
		self.avgpool = nn.AvgPool2d(7, stride=1)
		self.fc = nn.Linear(512, num_classes)
		
	def _make_layer(self, block, planes, blocks, stride=1):
		downsample = None
		if stride != 1 or self.inplanes != planes:
			downsample = nn.Sequential(
				nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),
				nn.BatchNorm2d(planes),
				)
		layers = []
		layers.append(block(self.inplanes, planes, stride, downsample))
		self.inplanes = planes
		for i in range(1, blocks):
			layers.append(block(self.inplanes, planes))
		return nn.Sequential(*layers)
	
	def forward(self, x):
		x = self.conv1(x)
		x = self.maxpool(x)
		x = self.layer0(x)
		x = self.layer1(x)
		x = self.layer2(x)
		x = self.layer3(x)
		x = self.avgpool(x)
		x = x.view(x.size(0), -1)
		x = self.fc(x)
		return x
	
model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)
\end{python}

\subsection{GoogLeNet}
\todo{}

\subsection{U-Net}
The U-Net architecture by \citeaus{ronneberger2015u} can also be viewed as an encoder-decoder architecture \cite{kingma2013auto} with skip connections of \texttt{ResNet} (\figref{fig:unet1}). This structure is later adopted in \texttt{pix2pix} architecture for image-to-image translation \cite{isola2017image}.
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.9\textwidth]{unet.png}
	\caption{The U-Net architecture. \cite{ronneberger2015u}}
\end{figure}
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.35\textwidth]{unet1.png}
	\caption{The U-Net as a variational auto-encoder with skip connections. \cite{isola2017image}}
	\label{fig:unet1}
\end{figure}

\todo{benefits, comments?}

\subsection{DenseNet}
\todo{} \citeaustitle{huang2017densely}

\subsection{Comparison between Networks}
\begin{figure}[hbt!]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{top1accuracy-nets.png}
		\captionof{figure}{Top1 \ac{vs} network.}
		\label{fig:top1accuracy-nets}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[width=0.95\textwidth]{top1accuracy-size-nets.png}
		\captionof{figure}{Top1 \ac{vs} operations, size $\propto$ \ac{param}.}
		\label{fig:top1accuracy-size-nets}
	\end{minipage}
\end{figure}

\section{Structures for Sequential Data}
\subsection{Recurrent Neural Network}
The next three models deal with sequential data. It's a bit uncertain what is the original paper proposing the idea though \cite{elman1990finding}. \ac{BPTT}:\\
\todo{Add image, content}
\begin{align}
	\frac{\partial E_t}{\partial w_{ij}} &= \sum_{1 \leq k \leq t} \left( \frac{\partial E_t}{\partial h_t} \frac{\partial h_t}{\partial h_k} \frac{\partial h_k}{\partial w_{ij}} \right)\\
	E &= \sum_{1 \leq t \leq T} E_t\\
	\frac{\partial h_t}{\partial h_k} &= \prod_{t \geq i \geq k} \frac{\partial h_i}{\partial h_{i-1}}
\end{align}

\subsection{LSTM}
\cite{sutskever2014sequence} \todo{}

\subsection{Transformer}
\ac{RNN} suffers from vanishing/exploding gradient. \ac{LSTM} suffers from slow training time. \citeausm{vaswani2017attention} propose a approach based on the idea of attention. This approach can utilize the computation power of \ac{GPU} for parallelized matrix computation. This technique originally is meant for \ac{NLP} problem, but later on, also show meaningful application in \ac{CV}.

\begin{itemize}
	\item A easy and detailed explanation from \citeaus{alammar2020}
	This idea is the basis for GPT-3 \cite{brown2020language} and BERT \cite{devlin2018bert}
\end{itemize}
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.5\textwidth]{transformer.png}
	\caption{The Transformer - model architecture \cite{vaswani2017attention}.}
\end{figure}

\begin{figure}[hbt!]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[width=0.55\textwidth]{transformer-1.png}
		\captionof{figure}{Scaled Dot-Product Attention.}
	\end{minipage}%
	\begin{minipage}{.45\textwidth}
		\centering
		\includegraphics[width=0.82\textwidth]{transformer-2.png}
		\captionof{figure}{Multi-Head Attention.}
	\end{minipage}
\end{figure}

\section{Bayesian Neural Network}
Deep neural network's strength is its ability to approximate function. However, as powerful as it is, there is also a threat to overfit to the training data and fail to generalize on the test set. There are various techniques to reduce overfit and improve transfer learning, \eg, weight regularization, dropout, batch norm.

A conventional neural network takes in the same input and produce the same output. A Bayesian neural network different output when it takes in the same inputs twice. The resulting algorithm:
\begin{itemize}
	\item mitigates overfitting
	\item enables learning from small datasets
	\item tells us how uncertain our predictions are.
	\item adds stochasticity to neural network.
	\item Instead of weights, it has weights with distribution
	\begin{align}
		w_i \sim \mathcal{N}(\mu_i, \sigma_i^2)
	\end{align}
\end{itemize}

\note There is a thing called Bayesian Network, \ac{aka} belief network, which is not the same as Bayesian neural network.

\begin{align*}
	\mathcal{D}_{tr} = \{\textbf{x}_i, y_i\}^n_{i=1}, \quad \text{model } F_\theta, \quad \text{loss } \mathcal{L}
\end{align*}
\begin{center}
	\begin{tabular}{p{7cm}|p{8cm}}
		Neural Network & Bayesian Neural Network\\ \hline \hline
		Training:
		\[\theta^* = \underset{\theta}{\arg\max} \sum_{(\textbf{x}_i, y_i)} \log [p(y_i | \textbf{x}_i, \theta)]\]
		\[\theta^* = \underset{\theta}{\arg\min} \sum_{(\textbf{x}_i, y_i)} \mathcal{L}(F_\theta(\textbf{x}_i), y_i)\] & Training:
		\[\mu^*, \Sigma^* = \underset{\mu, \Sigma}{\arg\max} \sum_{(\textbf{x}_i, y_i)} \log [p(y_i | \textbf{x}_i, \theta)] - KL[p(\theta), p(\theta_0)] \]
		\[\theta \sim \mathcal{N}(\mu, \Sigma), \qquad \theta_0 \sim \mathcal{N}(\textbf{0, I})\]
		\[\mu^*, \Sigma^* = \underset{\mu, \Sigma}{\arg\min} \sum_{(\textbf{x}_i, y_i)} \mathcal{L}(F_\theta(\textbf{x}_i), y_i) + KL[p(\theta), p(\theta_0)] \]
		\\ \hline
		Prediction:
		\[p(\hat{y} | \hat{\textbf{x}}, \theta^*)\]
		\[\hat{y} = F_{\theta^*}(\hat{\textbf{x}})\] & Prediction:
		\[p(\hat{y} | \hat{\textbf{x}}, \mathcal{D}_{tr}) = \int p(\hat{y} | \hat{\textbf{x}}, \theta^*) p(\theta^* | \mathcal{D}_{tr}) d\theta^* \]
		\[\theta^* \sim \mathcal{N}(\mu^*, \Sigma^*)\]
		\[\hat{y} = \frac{1}{K} \sum_{k=1}^K F_{\theta^*_k} (\hat{\textbf{x}}), \qquad \theta^*_k \sim \mathcal{N}(\mu^*, \Sigma^*)\]
	\end{tabular}
\end{center}

\subsection{References}
Examples:
\begin{itemize}
	\item \href{https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/}{cs.toronto}
	\item \href{https://keras.io/examples/keras_recipes/bayesian_neural_networks/}{Keras's example}
	\item \href{https://github.com/Harry24k/bayesian-neural-network-pytorch}{BNN-pytorch}
	\item \href{https://github.com/kumar-shridhar/PyTorch-BayesianCNN}{PyTorch-BayesianCNN}
\end{itemize}

If you want to just get the high level idea, watch these:
\begin{itemize}
	\item \href{https://youtu.be/s0S6HFdPtlA}{YouTube/PyData}
\end{itemize}

References: \note very long and complicate papers, I don't read them
\begin{itemize}
	\item \citeaustitle{jospin2022hands}
	\item \citeaustitle{shridhar2019comprehensive}
	\item \citeaustitle{liu2018adv}
\end{itemize}

\section{Graph Neural Networks}
\subsection{Introduction}
Many systems and interactions can be represented as graphs. \ac{GNN} are neural networks that operate on graph-structured data. Examples:

\begin{figure}[hbt!]
	\centering
	\begin{minipage}{.47\textwidth}
		\centering
		\includegraphics[width=\textwidth]{molecule-gnn.png}
		\captionof{figure}{Caffeine molecule structure as a graph. \cite{sanchez2021a}}
	\end{minipage}
	\hfil
	\begin{minipage}{.47\textwidth}
		\centering
		\includegraphics[width=\textwidth]{social-network-gnn.png}
		\captionof{figure}{A social network as a graph. \cite{sanchez2021a}}
	\end{minipage}
\end{figure}

Image and text can also be modeled as graphs, but it would be redundant, since they have fixed structures. Pixels in image are connected in a grid and words in a sentence are connected only with the prior and next ones.

There are three general types of prediction tasks on graphs: \cite{sanchez2021a}
\begin{itemize}
	\item graph-level: predict a single property for a whole graph. This is analogous to classification / regression problem on MNIST or CIFAR datasets.\\
	\Eg: Given a graph representing a molecule, predict the smell of it.
	\item node-level: predict some property for each node in a graph. This is analogous to segmentation problem of \ac{CV}, in which we predict the class for each pixel.\\
	\Eg: Given a graph representing the social network, predict the famous score of a person (a node)
	\item edge-level: predict the property or presence of edges in a graph.\\
	\Eg: Given a image of different peoples, objects, predict the interactions between them (\figref{fig:edge-level-task-GNN})
\end{itemize}

There are also
\begin{itemize}
	\item Node clustering task
	\item Link Prediction: Predicting missing links.
	\item Influence Maximization: Identifying influential nodes.
\end{itemize}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=0.79\textwidth]{edge-level-task-GNN.png}
	\caption{Example of edge-level prediction task \cite{sanchez2021a}.}
	\label{fig:edge-level-task-GNN}
\end{figure}

\subsection{Challenges}
\note Batch learning is difficult the setup

\subsection{Graph Representations in Machine Learning}
\begin{itemize}
	\item Using adjacency matrix is not permutation-invariant. (\figref{fig:graph-permutation})
	\begin{figure}[hbt!]
		\centering
		\includegraphics[width=0.79\textwidth]{graph-permutation.png}
		\caption{Different adjacency matrices represent the same graph \cite{sanchez2021a}.}
		\label{fig:graph-permutation}
	\end{figure}
	\item Another option is adjacency list
	\begin{itemize}
		\item Nodes: list of feature vectors for each node, tensor size $[n_{nodes}, dim_{node}]$
		\item Edges: list of feature vectors for each edge, tensor size $[n_{edges}, dim_{edge}]$
		\item Adjacency list: list of edges, tensor size $[n_{edges}, 2]$
		\item Global: a single feature vectors for global value, tensor size $[1, dim_{global}]$
	\end{itemize}
	\item Feature vector is also called as representation or embedding
	\Eg: feature vector for a person node in a social network: $\begin{bmatrix}
		age\\
		job\\
		marriage\ status\\
		nationality\\
		location
	\end{bmatrix}$
\end{itemize}

\subsection{GNN Block}
Between layers in \ac{GNN}, there are 3 \ac{MLP} to update graph attributes: feature vectors of nodes, edges and global value. \note The adjacency list stays unchanged.
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{gnn-block.png}
	\caption{A simple \ac{GNN} block. At layer $N$, the current graph $(V_n, E_n, U_n)$ is updated to $(V_{n+1}, E_{n+1}, U_{n+1})$, in which $V_n$ is the list of feature vectors for nodes, $E_n$ is the list of feature vectors for edges, and $U_n$ is global value. The three \ac{MLP} are $f_{U_n}$, $f_{V_n}$, $f_{E_n}$ correspond to each mentioned graph attributes.}
	\label{fig:gnn-block}
\end{figure}

We of course can have multiple of these blocks, similarly to have multiple hidden layers in a \ac{MLP}.
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{gnn-blocks.png}
	\caption{Multiple \ac{GNN} blocks before prediction network.}
	\label{fig:gnn-blocks}
\end{figure}

\subsection{GNN Predictions}
For specific prediction task, an additional \ac{MLP} operates with corresponding graph attribute.
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{gnn-prediction.png}
	\caption{Example of node-level prediction task: A \ac{MLP} $C_{V_{i,n}}$ produces prediction score on each node.}
	\label{fig:gnn-prediction}
\end{figure}

\subsection{Pooling Information}
Information may not be sufficient and complete in graph
\begin{itemize}
	\item Having only edge-level features, need to make node-level predictions.
	\item Having only node-level features, need to make edge-level predictions.
\end{itemize}

In these cases, the information can be transfer via pooling operation:
\begin{enumerate}
	\item Embeddings are gathered and concatenate into a matrix.
	\item The output embedding are then aggregated from the matrix, usually via a sum operation (\figref{fig:gnn-pooling}).
\end{enumerate}
\begin{figure}[hbt!]
	\centering
	\begin{subfigure}[b]{0.47\textwidth}
		\centering
		\includegraphics[width=\textwidth]{gnn-pooling.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.47\textwidth}
		\centering
		\includegraphics[width=\textwidth]{gnn-pooling-1.png}
	\end{subfigure}
	\caption{Example of pooling operation via sum. The feature embeddings of neighboring edges are aggregated to the node embedding, via a sum operation.}
	\label{fig:gnn-pooling}
\end{figure}
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{gnn-pooling-edge2node.png}
	\caption{Pooling operation from edge to node $\rho_{E_n \rightarrow V_n}$ before applying node-level prediction $C_{V_n}$.}
	\label{fig:gnn-pooling-edge2node}
\end{figure}

\subsection{Message Passing}
In \ac{GNN}, while pooling implies transferring information from one graph attribute to another (\eg, edge to node), message passing implies exchanging information and influence within the same graph attribute (\eg, node to node). The procedure, however, is similar to pooling.
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{gnn-message-passing.png}
	\caption{Message passing in graph: $\rho_{V_n \rightarrow V_{n+1}}$}
	\label{fig:gnn-message-passing}
\end{figure}

Combining different embeddings from different graph attributes can all be considered as a conditioning function:
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{gnn-conditioning.png}
	\caption{Schematic for conditioning one node's embedding based on three other embeddings (adjacent nodes, adjacent edges, global).}
	\label{fig:gnn-conditioning}
\end{figure}

\begin{figure}[hbt!]
	\centering
	\begin{minipage}{.47\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{gnn-node-edge-learning.png}
		\captionof{figure}{Node then edge learning}
	\end{minipage}
	\hfil
	\begin{minipage}{.47\textwidth}
		\centering
		\includegraphics[width=.6\textwidth]{gnn-edge-node-learning.png}
		\captionof{figure}{Edge then node learning}
	\end{minipage}
\end{figure}
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=.9\textwidth]{gnn-weave.png}
	\caption{Learning in "weave fashion".}
\end{figure}
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=.9\textwidth]{gnn-graph-net.png}
	\caption{Schematic of a Graph Nets architecture leveraging global representations.}
\end{figure}

\subsection{Some Empirical Design Lessons}
Source: \citeaustitle{sanchez2021a}
\begin{itemize}
	\item A higher number of parameters does correlate with higher performance.
	\item \ac{GNN}s are a very parameter-efficient model type: for even a small number of parameters
	\item Models with higher dimensionality or more layers tend to have better mean and lower bound performance, but the same trend is not found for the maximum.
	\item \ac{GNN} with a higher number of layers will broadcast information at a higher distance and can risk having their node representations ‘diluted’ from many successive iterations
	\item Sum has a very slight improvement on the mean performance, but max or mean can give equally good models.
	\item The more type of message passing the model has, the better the performance is.
\end{itemize}

\note
\begin{itemize}
	\item Graph is very scalable. Interaction between edges and nodes in a very large graph can be trained within a smaller graph \cite{pfaff2020learning}
	\item Take advantage of inductive biases (\href{https://youtu.be/dtPcqEnLvqc}{Learning physical simulation with graph network})
\end{itemize}

\subsection{Application}
Source: \href{https://youtu.be/p2aqXKfRXEA}{CS224W}
\begin{itemize}
	\item PinSAGE: \ac{GNN} for Recommendation systems: nodes are users, products. Based on other users' interactions with some products, predict whether one user would interest in a new product.
	\item Decagon: heterogeneous \ac{GNN} for drug prediction
	\item Goal-directed generation: GCPN for molecule generation
	\item Mesh-based simulation \cite{pfaff2020learning}
\end{itemize}

\subsection{References}
\begin{itemize}
	\item \href{https://youtu.be/bA261BF0bdk}{Siraj Raval YouTube}
	\item \citeaustitle{sanchez2021a}
	\item \citeaustitle{daigavane2021understanding}
	\item \href{https://gordicaleksa.medium.com/how-to-get-started-with-graph-machine-learning-afa53f6f963a}{How to get started with Graph Machine Learning - Aleksa Gordić}
\end{itemize}

There are so many current open problems for research (2022)
\begin{itemize}
	\item Convolutions on Graphs \cite{daigavane2021understanding}
	\item Multi-graphs
	\item Hyper-graphs
	\item Hyper-nodes
	\item hierarchical graphs
	\item Graph convolutions
	\item Graph Attention Networks
	\item Generative modeling
\end{itemize}