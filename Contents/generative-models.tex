% !TeX spellcheck = en_US
\chapter{Generative Models}

\section{Definitions}
\label{sec:vae-defs}
\begin{itemize}
	\item \textit{Probabilistic model}: is a model that represents a probability distribution. It could be a marginal \ac{prob} distribution $p(x)$ or a conditional \ac{prob} distribution $p(y|x)$.
	\item \textit{Evidence, query, and latent variables}:\\
	The word \hlb{latent} implies \hlb{existing, but hidden}. \Eg:
	\begin{itemize}
		\item In $p(x)$, $x$ is the query variable and there is no evidence variable.
		\item In $p(y|x)$, $y$ is the query variable and $x$ is the evidence variable.
		\item In \ac{MoG} $p(x) = \sum_z p(x|z) p(z)$, $x$ is the query variable, $z$ is the latent variable to represent the mixture elements.		
		\item In $p(y|x) = \sum_z p(y|x,z) p(z)$, $z$ is the latent variable.
		\item In a neural network, the inputs are evidence variables, the output are the query variables. In \ac{VAE}, $z$ is considered as the latent variable with a simple Gaussian distribution.
		\item In model-based \ac{RL} with latent variable models, state $x_t$ is the latent variable.
	\end{itemize}
	\item \textit{Latent variable models}: is the type of probabilistic model that represent a complex \ac{pdf} as the product of multiple simpler \ac{pdf}s, one of which belongs to a latent variable.\\
	\Eg: in \ac{VAE}, the complex distribution $p(x)$ (of an image) is represented as the product of two simple distribution. $p(z)$ is a Gaussian distribution, and $p(x|z)$ as a learnable neural network
	\[ p(x) = \int p(x|z) p(z) dz \]
\end{itemize}

\section{Variational Inference}
\hlb{Goal:} to model a complex distribution $p_\theta(x)$, given the data $\mathcal{D} = \{ x_1, \dots, x_N \}$
\begin{align}
	&\text{Maximum likelihood fit:} && \theta \leftarrow \underset{\theta}{\arg\max} \frac{1}{N} \sum_i \log p_\theta(x_i)\\
	&\text{With latent variable $z$:} && p(x) = \int p(x|z) p(z) dz\\
	\Rightarrow &\text{(sadly, completely \hlb{intractable})} && \theta \leftarrow \underset{\theta}{\arg\max} \frac{1}{N} \sum_i \log \left( \int p(x|z) p(z) dz \right)\\
	\Rightarrow &\text{alternative: \textit{expected} log-likelihood} && \theta \leftarrow \underset{\theta}{\arg\max} \frac{1}{N} \mathbb{E}_{z\sim p(z|x_i)} [\log p_\theta (x_i, z)]
\end{align}
This section describes how to calculate $p(z|x_i)$

\subsection{The Variational Approximation}
\hlb{Idea:} Approximating $p(z|x_i)$ with $q_i(z) = \mathcal{N}(\mu_i, \sigma_i)$

\begin{align}
	\log p(x_i) &= \log \int_z p(x_i|z) p(z) dz\\
	&= \log \int_z p(x_i|z) p(z) \frac{q_i(z)}{q_i(z)} dz\\
	&= \log \mathbb{E}_{z\sim q_i(z)} \left[ \frac{p(x_i|z) p(z)}{q_i(z)} \right]\\
	&\geq \mathbb{E}_{z\sim q_i(z)} \left[ \log \frac{p(x_i|z) p(z)}{q_i(z)} \right] \qquad(\text{Jensen's inequality: $\log \mathbb{E}[y] \geq \mathbb{E}[\log y]$})\\
	&= \mathbb{E}_{z\sim q_i(z)} [\log p(x_i|z) + \log p(z)] - \mathbb{E}_{z\sim q_i(z)}[\log q_i(z)]\\
	&= \underbrace{\mathbb{E}_{z\sim q_i(z)} [\log p(x_i|z) + \log p(z)] + \mathcal{H}(q_i)}_{\textstyle \mathcal{L}_i (p, q_i)}
\end{align}

\begin{itemize}
	\item $\mathbb{E}_{z\sim q_i(z)} [\log p(x_i|z) + \log p(z)] + \mathcal{H}(q_i)$ is the lower bound of $\log p(x_i)$.\\
	Thus, maximize $\mathbb{E}_{z\sim q_i(z)} [\log p(x_i|z) + \log p(z)] + \mathcal{H}(q_i)$ will indirectly maximize $\log p(x_i)$.
	\item Maximize the 1st term $\mathbb{E}_{z\sim q_i(z)} [\log p(x_i|z) + \log p(z)]$ will find the place with high $p(x_i, z)$\\
	Maximize the 2nd term $\mathcal{H}(q_i)$ will increase the randomness of $z$
\end{itemize}

In another point of view, as we approximate $p(z|x_i)$ with $q_i(z)$, how good is the approximation can be measured with the \ac{KL}-divergence $D_{KL}(q_i(z) || p(z|x_i))$:

\begin{align}
	D_{KL}(q_i(z) || p(z|x_i)) &= \mathbb{E}_{z\sim q_i(z)} \left[ \log\frac{q_i(z)}{p(z|x_i)} \right] = \mathbb{E}_{z\sim q_i(z)} \left[ \log\frac{q_i(z)p(x_i)}{p(x_i, z)} \right]\\
	&= - \mathbb{E}_{z\sim q_i(z)} [ \log p(x_i|z) + \log p(z) ] + \mathbb{E}_{z\sim q_i(z)} [\log q_i(z)] + \mathbb{E}_{z\sim q_i(z)} [\log p(x_i)]\\
	&= - \underbrace{\mathbb{E}_{z\sim q_i(z)} [ \log p(x_i|z) + \log p(z) ] - \mathcal{H}(q_i)}_{\textstyle \mathcal{L}_i (p, q_i)} + \log p(x_i)\\
	\Rightarrow\; \log p(x_i) & = D_{KL}(q_i(z) || p(z|x_i)) + \mathcal{L}_i (p, q_i)\\
	\Rightarrow\; \log p(x_i) & \geq \mathcal{L}_i (p, q_i), \qquad \text{since } D_{KL}(q||p) \geq 0
\end{align}

Since $\log p(x_i)$ is independent of $q_i$, maximizing $\mathcal{L}_i (p, q_i)$ \ac{wrt} $q_i$ will minimize the \ac{KL}-divergence, thus, improving the approximation.

\subsection{Algorithm}
Again, the goal is to model the complex distribution $p(x)$. Since the common maximum likelihood is intractable, we maximize its lower bound.
\begin{align*}
	& \theta \leftarrow \underset{\theta}{\arg\max} \frac{1}{N} \sum_i \log p_\theta(x_i) &&- \text{intractable maximum likelihood}\\
	& \theta \leftarrow \underset{\theta}{\arg\max} \frac{1}{N} \sum_i \mathcal{L}_i(p, q_i) &&- \text{tractable lower bound}
\end{align*}

\hlb{The algorithm:}
\begin{enumerate}
	\item for each $x_i$ (or mini-batch):
	\item \quad calculate $\nabla_\theta \mathcal{L}_i (p, q_i)$:
	\item \qquad sample $z \sim q_i(z)$
	\item \qquad $\nabla_\theta \mathcal{L}_i (p, q_i) \approx \nabla_\theta \log p_\theta (x_i | z)$
	\item \quad $\theta \leftarrow \theta + \alpha \nabla_\theta \mathcal{L}_i (p, q_i)$
	\item \quad update $q_i$ to maximize $\mathcal{L}_i (p, q_i)$
\end{enumerate}

In the above $6^{th}$ step, assuming $q_i(z) = \mathcal{N}(\mu_i, \sigma_i)$:
\begin{itemize}
	\item calculate the gradient $\nabla_{\mu_i} \mathcal{L}_i (p, q_i)$ and $\nabla_{\sigma_i} \mathcal{L}_i (p, q_i)$
	\item apply gradient ascent on $\mu_i$ and $\sigma_i$ to maximize $\mathcal{L}_i (p, q_i)$.
\end{itemize}

\hlb{Problem:} for each $x_i$, there will be one $\mu_i$ and one $\sigma_i$, with the appropriate dimension. Thus, the total number of \ac{param} would be huge: $|\theta| + (|\mu_i| + |\sigma_i|) \times N$

\section{Amortized Variational Inference}
As mentioned above, the problem with using a Gaussian for $q_i(z)$ is the huge number of \ac{param}. The word \textit{amortize} means to reduce or pay off (a debt) with regular payments. \textit{Amortized Variational Inference} alleviates the \ac{param} problem by using a neural network (\figref{fig:amortized-variational-inference})
\begin{align}
	&q_i(z) = q(z | x_i) \approx q_\phi(z|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi(x))\\
	\Rightarrow\;& \log p(x_i) \geq \underbrace{\mathbb{E}_{z\sim q_\phi(z|x_i)} [ \log p_\theta(x_i|z) + \log p(z) ] + \mathcal{H}(q_\phi(z|x_i))}_{\textstyle \mathcal{L}_i (p_\theta(x_i|z), q_\phi(z|x_i))}
\end{align}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{amortized-variational-inference.png}
	\caption{Two neural networks: $p_\theta(x|z)$ and $q_\phi(z|x)$.}
	\label{fig:amortized-variational-inference}
\end{figure}

\subsection{Algorithm}
\hlb{The algorithm:}
\begin{enumerate}
	\item for each $x_i$ (or mini-batch):
	\item \quad calculate $\nabla_\theta \mathcal{L}_i (p_\theta(x_i|z), q_\phi(z|x_i))$:
	\item \qquad sample $z \sim q_\phi(z|x_i)$
	\item \qquad $\nabla_\theta \mathcal{L} \approx \nabla_\theta \log p_\theta(x_i|z)$
	\item \quad $\theta \leftarrow \theta + \alpha \nabla_\theta \mathcal{L}$
	\item \quad $\phi \leftarrow \phi + \alpha \nabla_\phi \mathcal{L}$
\end{enumerate}

Let's consider the gradient $\nabla_\phi \mathcal{L}$ in the above $6^{th}$ step:
\begin{align}
	\mathcal{L}_i &= \underbrace{\mathbb{E}_{z\sim q_\phi(z|x_i)} \big[\log p_\theta(x_i|z) + \log p(z)\big]}_{\textstyle J(\phi) = \mathbb{E}_{z\sim q_\phi(z|x_i)} \big[ r(x_i, z)\big]} + \mathcal{H}(q_\phi(z|x_i))\\
	&= \nabla_\phi J(\phi) + \nabla_\phi \mathcal{H}(q_\phi(z|x_i))
\end{align}

\begin{itemize}
	\item There is a formula to find the gradient of the entropy of a Gaussian
	\begin{equation}
		\nabla_\phi \mathcal{H}(q_\phi(z|x_i)) = \nabla_\phi \mathcal{H} \big(\mathcal{N}(\mu_\phi, \sigma_\phi)\big)
	\end{equation}	
	\item To find the gradient $\nabla_\phi J(\phi)$, we could use similar approach as in policy gradient, however, with the problem of high variance. Instead, we could use the reparameterization trick:
	\begin{align}
		J(\phi) & = \mathbb{E}_{z\sim q_\phi(z|x_i)} \big[ r(x_i, z)\big] && q_\phi(z|x) = \mathcal{N}(\mu_\phi(x), \sigma_\phi(x))\\
		& = \mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)} \big[ r(x_i, \mu_\phi(x) + \epsilon \sigma_\phi(x))\big] && z = \mu_\phi(x) + \epsilon \sigma_\phi(x)
	\end{align}
	with $\epsilon \sim \mathcal{N}(0,1)$ is independent of $\phi$!\\
	$\Rightarrow$ estimate $\nabla_\phi J(\phi)$:
	\begin{itemize}
		\item sample $\epsilon_1, \dots, \epsilon_M$ from $\mathcal{N}(0,1)$ (a single sample works just well!)
		\item $\nabla_\phi J(\phi) \approx \frac{1}{M} \sum_j \nabla_\phi r(x_i, \mu_\phi(x) + \epsilon \sigma_\phi(x))$
	\end{itemize}
\end{itemize}

\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{VAE.png}
	\caption{The complete structure for amortized variational inference.}
	\label{fig:vae}
\end{figure}

\subsection{Reparameterization Trick}
Another way to look:
\begin{align}
	\mathcal{L}_i &= \mathbb{E}_{z\sim q_\phi(z|x_i)} \big[\log p_\theta(x_i|z) + \log p(z)\big] + \mathcal{H}(q_\phi(z|x_i))\\
	&= \mathbb{E}_{z\sim q_\phi(z|x_i)} [\log p_\theta(x_i|z)] + \mathbb{E}_{z\sim q_\phi(z|x_i)}[\log p(z)] + \mathcal{H}(q_\phi(z|x_i))\\
	&= \mathbb{E}_{z\sim q_\phi(z|x_i)} [\log p_\theta(x_i|z)] -D_{KL} (q_\phi(z|x_i) || p(z))\\
	&= \mathbb{E}_{\epsilon \sim \mathcal{N}(0,1)} \big[\log p_\theta(x_i|\mu_\phi(x) + \epsilon \sigma_\phi(x))\big] -D_{KL} (q_\phi(z|x_i) || p(z))\\
	&\approx \log p_\theta(x_i|\mu_\phi(x) + \epsilon \sigma_\phi(x)) -D_{KL} (q_\phi(z|x_i) || p(z)) \quad \text{(single sample estimate)}
\end{align}

\note Reparameterization trick \ac{vs} policy gradient
\begin{itemize}
	\item Policy Gradient
	\[\nabla_\phi J(\phi) \approx \frac{1}{M} \sum_j \nabla_\phi \log q_\phi(z_j|x_i) r(x_i, z_j) \]
	$\begin{matrix*}[l]
		\color{Green} + \text{can handle both discrete and continuous latent variables}\\
		\color{red} - \text{high variance, requires multiple samples \& small learning rates}
	\end{matrix*}$
	\item Reparameterization trick
	\[\nabla_\phi J(\phi) \approx \frac{1}{M} \sum_j \nabla_\phi r(x_i, \mu_\phi(x) + \epsilon \sigma_\phi(x))\]
	$\begin{matrix*}[l]
		\color{red} - \text{only continuous latent variables}\\
		\color{Green} + \text{very simple to implement}\\
		\color{Green} + \text{low variance}
	\end{matrix*}$
\end{itemize}

\section{Conditional Variational Inference Models}
\label{sec:conditional-variational-inference-models}
\begin{equation}
	\mathcal{L}_i = \mathbb{E}_{z\sim q_\phi(z|x_i, y_i)} \big[\log p_\theta(y_i|x_i, z) + \log p(z|x_i)\big] + \mathcal{H}(q_\phi(z|x_i, y_i))\\
\end{equation}
\begin{figure}[hbt!]
	\centering
	\includegraphics[width=\textwidth]{conditional-VAE.png}
	\caption{The conditional model.}
	\label{fig:conditional-vae}
\end{figure}

\section{Generative Models}
\textit{Generative models} are models that generate data $x$. \Eg: $p(x)$ is a generative model, because knowing $p(x)$, we can sample $x$.

\note Not all generative models are not necessary latent variable models, and not all latent variable models are generative models. But it's common for a generative model to be a latent variable model, because to generate data, we usually want to know the \ac{prob} distribution of it. When that \ac{prob} distribution is complex, we would represent it as a product of multiple simple \ac{prob} distribution, using some latent variables.

\subsection{Generative Adversarial Network}
\ac{GAN} by \citeausm{goodfellow2014generative}:
\begin{align}
	&D(x) &&-\text{\ac{prob} that $x$ is real (from training data)}\\
	&1-D(x) &&-\text{\ac{prob} that $x$ is fake (from the generator)}\\
	&\underset{D}{\max}\, \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)] &&-\text{the discriminator goal}\\
	&\underset{G}{\min}\, \mathbb{E}_{z \sim p_z(z)} [\log (1-D(G(z)))] &&-\text{the generator goal}
\end{align}
\begin{itemize}
	\item The discriminator goal is to maximize the $\log$ \ac{prob} of the data from the training set classified as real.
	\item The generator goal is to minimize the $\log$ \ac{prob} of the data from the generator classified as fake.
\end{itemize}

The loss function of \ac{GAN}:
\begin{equation}
	\underset{G}{\min}\, \underset{D}{\max} V(D,G) = \underset{G}{\min}\, \underset{D}{\max} \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_z(z)} [\log (1-D(G(z)))]
\end{equation}

In \texttt{pytorch}, there is BCELOSS (Binary Cross Entropy loss):
\begin{align}
	&l(x,y) = L = \{l_1, \dots, l_N\}^T\\
	&l_n = - w_n [y_n \log x_n + (1-y_n) \log(1-x_n)]
\end{align}

\subsection{DCGAN}
\ac{DCGAN} by \citeaus{radford2015unsupervised} is the completely convolutional version of \ac{GAN}.
\begin{itemize}
	\item The discriminator has strided convolution, batch norm layers, Leaky \ac{relu} activations.
	\item The generator has transposed convolution, batch norm layers, \ac{relu} activations.
\end{itemize}

\todo{}

\subsection{CGAN}
\ac{CGAN} (based on \secref{sec:conditional-variational-inference-models}) is first proposed by \citeaus{mirza2014conditional}.

Example works:
\begin{itemize}
	\item pix2pix: image-to-image translation \cite{isola2017image}
	\begin{align}
		\texttt{bool} &= D(x,y) &&-\text{given $x$, the discriminator classify whether $y$ is real.}\\
		y &= G(x,z) &&-\text{given $x$ and noise $z$, the generator generate $y$.}
	\end{align}
\end{itemize}

\subsection{Tips And Tricks}
Tips and tricks for training \ac{GAN} from \href{https://github.com/soumith/ganhacks}{\texttt{github/ganhacks}}
\begin{itemize}
	\item Train the Discriminator: maximizing $\log D(x) + \log (1-D(G(z)))$\\
	Construct different mini-batches for real and fake, then accumulate the gradients
	\begin{itemize}
		\item Compute the loss $\log D(x)$ with real samples
		\item Compute the loss $\log (1 - D(G(z)))$ with fake samples
	\end{itemize}
	\item Train the Generator: maximizing $\log D (G(z))$\\
	Minimizing the $\log(1 - D(G(z)))$ leads to insufficient gradients, especially in early learning process. Thus, we instead maximize $\log D (G(z))$.
\end{itemize}

\subsection{Examples}
\begin{itemize}
	\item \ac{GAN}: \href{https://github.com/diegoalejogm/gans/blob/master/1.%20Vanilla%20GAN%20PyTorch.ipynb}{\texttt{pytorch}}, \href{https://github.com/diegoalejogm/gans/blob/master/1.%20Vanilla%20GAN%20TensorFlow.ipynb}{\texttt{tensorflow}} (MNIST dataset)
	\item \ac{DCGAN}:
	\begin{itemize}
		\item MNIST: \href{https://github.com/diegoalejogm/gans/blob/master/2.%20DC-GAN%20PyTorch-MNIST.ipynb}{\texttt{pytorch}}
		\item CIFAR10: \href{https://github.com/diegoalejogm/gans/blob/master/2.%20DC-GAN%20PyTorch.ipynb}{\texttt{pytorch}}, \href{https://github.com/diegoalejogm/gans/blob/master/2.%20DC-GAN%20TensorFlow.ipynb}{\texttt{tensorflow}}
		\item CelebA: \href{https://pytorch.org/tutorials/beginner/dcgan\_faces\_tutorial.html}{\texttt{Pytorch}'s tutorial}
	\end{itemize}
	\item \ac{CGAN}
	\begin{itemize}
		\item The CMP Facade Database: \href{https://www.tensorflow.org/tutorials/generative/pix2pix}{\texttt{tensorflow}'s tutorial}
	\end{itemize}
\end{itemize}

\section{References}
\begin{itemize}
	\item \href{https://youtu.be/UTMpM4orS30}{Variational Inference | CS 285, Stanford | YouTube}
	\item \href{https://youtu.be/5WoItGTWV54}{Generative models | CS231, Stanford | YouTube}
	\item \href{https://youtu.be/9zKuYvjFFS8}{Variational Autoencoders | Arxiv Insights | YouTube}
	\item \href{https://youtu.be/fcvYpzHmhvA}{Variational Autoencoders | CodeEmporium | YouTube}
	\item \href{https://github.com/vdumoulin/conv_arithmetic}{Real "deconv" layer | GitHub}
\end{itemize}